{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each f output based on input x\n",
    "# target function == f(x)\n",
    "possibleTargetFuncOutputs = np.array([\n",
    "    [0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]\n",
    "])\n",
    "\n",
    "# input\n",
    "xOutsideD = np.array(\n",
    "    [\n",
    "        [1,0,1], [1,1,0], [1,1,1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "answerSelections = {\n",
    "    \"a\": 0,\n",
    "    \"b\": 0,\n",
    "    \"c\": 0,\n",
    "    \"d\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fAndGNumAgreements(g, boolVectors: np.ndarray, possibleTargetFuncOutputs: np.ndarray, maxDisagreements: int):\n",
    "    # add max agreements parameter?\n",
    "    # for target fuc output\n",
    "    # bothFucAgreed = False\n",
    "    # maxDisagreements = ? 0, 1, 2, 3,\n",
    "    # for output value\n",
    "    # expectedAndHypotAgreed = False\n",
    "    # x-vector outside of D input into g(x)\n",
    "    # if g(x) == expected f(x) output value\n",
    "    #   expectedAndHypotAgreed = True\n",
    "    # else expectedAndHypotAgreed = False\n",
    "    #  if not expectedAndHypotAgreed and maxDisagreements 0:\n",
    "    #   break/return False maybe bothFucAgreed = False\n",
    "    # \n",
    "    # \n",
    "    # outer loop if bothFucAgreed:\n",
    "    #   numOfAgreementFunc += 1\n",
    "    # f(x) already computed and is the matrix of target function outputs\n",
    "    numOfAgreementFunc = 0\n",
    "    for fucOutput in possibleTargetFuncOutputs:\n",
    "        bothFucAgreed = False\n",
    "        numOfAgreements = 0\n",
    "        # currentDisagreements = 0\n",
    "        for i, output in enumerate(fucOutput):\n",
    "            bothFucAgreed = g(boolVectors[i]) == output\n",
    "            if bothFucAgreed:\n",
    "                numOfAgreements += 1\n",
    "        if numOfAgreements == (3 - maxDisagreements):\n",
    "            numOfAgreementFunc += 1\n",
    "    \n",
    "    return numOfAgreementFunc\n",
    "\n",
    "\n",
    "def scoreG(g, boolVectors: np.ndarray, possibleTargetFuncOutputs: np.ndarray):\n",
    "    finalScore = 0\n",
    "    numPoints = boolVectors.ndim\n",
    "    # loop 3 (i) times to score start @ 1? (skip the agreements with 0 since x * 0 = 0)\n",
    "    # call fAndGNumAgreements with max disagreements of i\n",
    "    # then finalScore += fAndGNumAgreements * i\n",
    "    # return finalScore\n",
    "    numOfDisagreements = 0\n",
    "    while numOfDisagreements < 3:\n",
    "        finalScore += fAndGNumAgreements(g, boolVectors, possibleTargetFuncOutputs, numOfDisagreements) * numOfDisagreements\n",
    "        numOfDisagreements += 1\n",
    "    return finalScore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# a\n",
    "# returns 1 on all vectors\n",
    "def gA(threeDBoolVector):\n",
    "    return 1\n",
    "\n",
    "\n",
    "print(scoreG(gA, xOutsideD, possibleTargetFuncOutputs))\n",
    "answerSelections[\"a\"] = scoreG(gA, xOutsideD, possibleTargetFuncOutputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# b\n",
    "# returns 0 on all vectors\n",
    "def gB(threeDBoolVector):\n",
    "    return 0\n",
    "\n",
    "\n",
    "print(scoreG(gB, xOutsideD, possibleTargetFuncOutputs))\n",
    "answerSelections[\"b\"] = scoreG(gB, xOutsideD, possibleTargetFuncOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# c\n",
    "# XOR odd num of 1 = 1, even num  of 1 = 0\n",
    "from functools import reduce\n",
    "from operator import xor\n",
    "\n",
    "# takes the XOR cumulatively starting with first 2 elements then XOR results with element 3.... and onwards\n",
    "def gC(threeDBoolVector: np.ndarray):\n",
    "    return reduce(xor, threeDBoolVector)\n",
    "\n",
    "print(scoreG(gC, xOutsideD, possibleTargetFuncOutputs))\n",
    "answerSelections[\"c\"] = scoreG(gC, xOutsideD, possibleTargetFuncOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# d\n",
    "# the opposite XOR odd num of 1 = 0, even num  of 1 = 1\n",
    "from functools import reduce\n",
    "from operator import xor\n",
    "\n",
    "# the inverse of XOR is XOR so just flipping the bits to reverse the function provided in option d in the HW 1\n",
    "#  1,0,1 => 0,1,0 thus returning the opposite of the XOR function in the end\n",
    "#    0        1\n",
    "def flipBit(bit: int):\n",
    "    return bit ^ 1\n",
    "\n",
    "# XOR elements cumulatively but flips the bits first to get the  opposite of the XOR\n",
    "def gD(threeDBoolVector: np.ndarray):\n",
    "    return reduce(xor, map(flipBit, threeDBoolVector))\n",
    "\n",
    "print(scoreG(gD, xOutsideD, possibleTargetFuncOutputs))\n",
    "answerSelections[\"d\"] = scoreG(gD, xOutsideD, possibleTargetFuncOutputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [e] They are all equivalent (equal scores for g in [a] through [d]).\n",
    "Correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take-Aways/notes**:\n",
    "\n",
    "In finding the disagreement probability between f & g, I approximated this value by generating a large dataset of separate random points.\n",
    "\n",
    "check: numbers 9-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Perceptron Learning Algorithm (in 2 dimensions)\n",
    "##### intial W = 0 vector to miss classify all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLA:\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, w: np.ndarray = np.array([[0,0,0]], dtype=\"float64\").T):\n",
    "        ''' The Perceptron Learning Algorithm (in 2 dimensions)\n",
    "        x: ndarray\n",
    "            all inputs as column vectors in a matrix\n",
    "            \n",
    "            We also treat x[n], where n is a column, as a column vector and modify it to become\n",
    "            x[n] = [x0, xi, · · · , xd]T, where the added coordinate x0 is fixed at x0 = 1.\n",
    "        y: ndarray\n",
    "            corresponding correct classification of a x[n], where n is a column\n",
    "        w: ndarray\n",
    "            a weight column vector\n",
    "\n",
    "            we will treat the bias b\n",
    "            as a weight wo = b and merge it with the other weights into one vector\n",
    "            w = [w0, w1 , · · · , wd]T thus w is column vector\n",
    "\n",
    "            defaults to the zero vector in 2-D if no intial weight vector is provided\n",
    "        '''\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        # for Learning from data HW #1 assignment\n",
    "        self.countWeightVectorUpdates = 0\n",
    "\n",
    "    def incrementCountWeightVectorUpdates(func):\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            self.countWeightVectorUpdates += 1\n",
    "            func(self, *args, **kwargs)\n",
    "        return wrapper\n",
    "    \n",
    "    # @ is matrix multiplication\n",
    "    def perceptronFormula(self, W: np.ndarray, X: np.ndarray) -> Literal[1,-1]:\n",
    "        '''\n",
    "        W: ndarray\n",
    "            Weight vector as column vector\n",
    "        X: ndarray\n",
    "            input column vector\n",
    "\n",
    "        h(x)/perceptron formula\n",
    "\n",
    "        ### Returns\n",
    "        y/class: ndarray\n",
    "            +1 or -1: \"where one side of the line maps to +1 and the other maps to -1.\" thus a binary classification???\n",
    "        '''\n",
    "        tempW = W\n",
    "        return np.sign(W.transpose() @ X).item()\n",
    "    \n",
    "    @incrementCountWeightVectorUpdates\n",
    "    def updateWeightVector(self, y: Literal[1,-1], X: np.ndarray):\n",
    "        '''\n",
    "        y: 1 | -1\n",
    "            the correct??? classification of the x input vector\n",
    "\n",
    "        x: ndarray\n",
    "            an input column vector\n",
    "        '''\n",
    "        # y correct classification \n",
    "        # X vector that maps to the y classification\n",
    "        self.w = self.w + (y * X).reshape((3,1))\n",
    "        #                  ^ scalar\n",
    "    \n",
    "    def train(self):\n",
    "        '''\n",
    "        Trains The Perceptron Learning Algorithm model based on provided x, y and w values provided during instantiation\n",
    "        '''\n",
    "        column = 0\n",
    "        # run until all points have been correctly classified\n",
    "        while column != self.x.shape[1]:\n",
    "            columnVector = self.x[:,column]\n",
    "            # if not (classification := self.y[:,column]) == self.perceptronFormula(self.w, columnVector):\n",
    "            # if current model is NOT able to correctly classify the point\n",
    "            if (classification := self.y[:,column]) != self.perceptronFormula(self.w, columnVector):\n",
    "                self.updateWeightVector(classification.item(), columnVector)\n",
    "                # run start check from beginning again\n",
    "                column = 0\n",
    "            else:\n",
    "                column += 1\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePLATestYs(w: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
    "    return np.sign(w.transpose() @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Random Line stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points in R 2\n",
    "def slopeOfTwoPoints(X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    return (Y[1] - Y[0])/(X[1] - X[0])\n",
    "\n",
    "# points in R 2\n",
    "def equationOfLineSlopInterceptForm(X: np.ndarray, Y: np.ndarray) -> dict[str, float]:\n",
    "    '''\n",
    "    y = mx + b\n",
    "    '''\n",
    "    line = {}\n",
    "    line[\"slope\"] = slopeOfTwoPoints(X, Y)\n",
    "    line[\"yIntercept\"] = -line[\"slope\"] * X[0] + Y[0]\n",
    "    return line\n",
    "\n",
    "\n",
    "def generateRandomLine(numOfPoints: int, lowerBound: int, upperBound: int):\n",
    "    # column 0 = x coordinates\n",
    "    # column 1 = y coordinates (each nested array is a point)\n",
    "    RandomPoints = np.random.uniform(lowerBound, upperBound, size=(numOfPoints,2))\n",
    "    return (equationOfLineSlopInterceptForm(RandomPoints[:,0], RandomPoints[:,1]), RandomPoints)\n",
    "\n",
    "# using vectorization for improved performance and practice\n",
    "def calculateYsForLine(X: np.ndarray, line: dict[str, float]) -> np.ndarray:\n",
    "    return (line[\"slope\"] * X) + line[\"yIntercept\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLA Experiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_x_y_data_set(numOfTrainingPoints: int, targetFInVectorForm: np.ndarray, dimension: int = 2, rngLowerBound: int = -1, rngUpperBound: int = 1):\n",
    "    # Generate random X values to train the algorithm\n",
    "    # generates some ground truth data using the target function f\n",
    "    # that the algorithm will use to find a function g that approximates??? or equals f\n",
    "\n",
    "    # might need to change this after intial testing\n",
    "    # is it random if random is seeded from a fixed seed?\n",
    "    randomState = np.random.RandomState(seed=0)\n",
    "\n",
    "    # each row is a vector\n",
    "    # thus columns are xi\n",
    "    randomPointsVectors = randomState.uniform(rngLowerBound, rngUpperBound, size=(numOfTrainingPoints, dimension))\n",
    "\n",
    "    '''\n",
    "    We also treat x as a column vector and modify it to become x =\n",
    "    [x0, xi, · · · , xd]T, where the added coordinate x0 is fixed at x0 = 1.\n",
    "\n",
    "    '''\n",
    "    x0 = np.ones(shape=randomPointsVectors.shape[0])\n",
    "\n",
    "    # append rows of ones that will be x0 for each vector\n",
    "    # make XTrain into column vectors in the matrix\n",
    "    XTrain = np.column_stack((x0, randomPointsVectors)).T\n",
    "\n",
    "    # YTrain are the classification the point belongs to (1 | -1)\n",
    "    YTrain = generatePLATestYs(w=targetFInVectorForm, x=XTrain)\n",
    "    return (XTrain, YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_f_and_g_Disagreement(f: np.ndarray, g: np.ndarray, numTestPoints: int):\n",
    "    XTest, YTest = generate_random_x_y_data_set(numTestPoints, f)\n",
    "    YPredict = generatePLATestYs(g, XTest)\n",
    "    # np.array_equal(YPredict, YTest)\n",
    "    disagreements = np.where(YPredict != YTest)\n",
    "    \n",
    "    disagreementsPercent = disagreements[0].shape[0] / numTestPoints\n",
    "\n",
    "    return disagreementsPercent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPLAExperiment(numRuns: int, numTrainPoints: int, numTestPoints: int, pointsINRunLowerBound: int = -1, pointsINRunUpperBound: int = 1, dimension: int = 2):\n",
    "    '''\n",
    "    In order to get a reliable estimate repeat the\n",
    "    experiment for 1000 runs\n",
    "\n",
    "    Used to answer HW # 7 - 10\n",
    "\n",
    "    returns: tuple[float, float]\n",
    "                index 0: average number of iterations it took The Perceptron Learning Algorithm model (g) to converge to random target f\n",
    "                index 1: average probability that f & g will disagree on their classification of a random point\n",
    "    '''\n",
    "    \n",
    "    totalMaxCountWeightVectorUpdatesPerRun = 0\n",
    "    disagreementsPercentPerRun = np.empty(numRuns)\n",
    "    # for loop the below\n",
    "    for iteration in range(numRuns):\n",
    "        fLine, fLineOriginPoints = generateRandomLine(2, pointsINRunLowerBound, pointsINRunUpperBound)\n",
    "\n",
    "        '''\n",
    "        we will treat the bias b\n",
    "        as a weight wo = b and merge it with the other weights into one vector\n",
    "        w = [w0, w1 , · · · , wd]T\n",
    "\n",
    "        We also treat x as a column vector and modify it to become x =\n",
    "        [x0, xi, · · · , xd]T, where the added coordinate x0 is fixed at x0 = 1.\n",
    "\n",
    "        '''\n",
    "        # equation of a line in \"standard form\" = x + y + b = 0 used to make column vector\n",
    "        # note \"-\" signs\n",
    "        targetFInVectorForm = np.array([\n",
    "            [\n",
    "                -fLine[\"yIntercept\"], # aka b\n",
    "                -fLine[\"slope\"], # x coefficient\n",
    "                1 # y coefficient\n",
    "            ]\n",
    "        ])\n",
    "\n",
    "        # make vector a column vector\n",
    "        targetFInVectorForm = targetFInVectorForm.T\n",
    "        XTrain, YTrain = generate_random_x_y_data_set(numTrainPoints, targetFInVectorForm)\n",
    "\n",
    "        # test PLA and train g to approximate f or equal\n",
    "        plaModel = PLA(x=XTrain, y=YTrain)\n",
    "        plaModel.train()\n",
    "        g = plaModel.w\n",
    "\n",
    "        # used for mean calculations to answer HW questions\n",
    "        disagreementsPercentPerRun[iteration] = approximate_f_and_g_Disagreement(targetFInVectorForm, g, numTestPoints)\n",
    "        totalMaxCountWeightVectorUpdatesPerRun += plaModel.countWeightVectorUpdates\n",
    "\n",
    "    meanConvergence = totalMaxCountWeightVectorUpdatesPerRun / numRuns\n",
    "    meanDisagreementsPercent = np.mean(disagreementsPercentPerRun)\n",
    "\n",
    "    return (meanConvergence, meanDisagreementsPercent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runPLAExperiment(1000, 10, -1, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create target function\n",
    "dimensions = 2\n",
    "targetFOriginalNumPoints = 2\n",
    "fPointLowerBound = -1\n",
    "fPointUpperBound = 1\n",
    "# XsVector = np.array([3,2])\n",
    "# YsVector = np.array([5,6])\n",
    "\n",
    "# line = equationOfLineSlopInterceptForm(XsVector, YsVector)\n",
    "\n",
    "line, lineOriginPoints = generateRandomLine(targetFOriginalNumPoints, fPointLowerBound, fPointUpperBound)\n",
    "\n",
    "'''\n",
    "we will treat the bias b\n",
    "as a weight wo = b and merge it with the other weights into one vector\n",
    "w = [w0, w1 , · · · , wd]T\n",
    "\n",
    "We also treat x as a column vector and modify it to become x =\n",
    "[x0, xi, · · · , xd]T, where the added coordinate x0 is fixed at x0 = 1.\n",
    "\n",
    "'''\n",
    "# equation of a line in \"standard form\" = x + y + b = 0 used to make column vector\n",
    "# note \"-\" signs\n",
    "targetFInVectorForm = np.array([\n",
    "    [\n",
    "        -line[\"yIntercept\"], # aka b\n",
    "        -line[\"slope\"], # x coefficient\n",
    "        1 # y coefficient\n",
    "    ]\n",
    "])\n",
    "\n",
    "# make vector a column vector\n",
    "targetFInVectorForm = targetFInVectorForm.T\n",
    "# targetFInVectorForm.T\n",
    "\n",
    "targetFInVectorForm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random X values to train the algorithm\n",
    "# generates some ground truth data using the target function f\n",
    "# that the algorithm will use to find a function g that approximates??? or equals f\n",
    "\n",
    "rngLowerBound = -1\n",
    "rngUpperBound = 1\n",
    "numOfTrainingPoints = 200\n",
    "\n",
    "randomState = np.random.RandomState(seed=0)\n",
    "\n",
    "# each row is a vector\n",
    "# thus columns are xi\n",
    "randomPointsVectors = randomState.uniform(rngLowerBound, rngUpperBound, size=(numOfTrainingPoints, dimensions))\n",
    "\n",
    "'''\n",
    "We also treat x as a column vector and modify it to become x =\n",
    "[x0, xi, · · · , xd]T, where the added coordinate x0 is fixed at x0 = 1.\n",
    "\n",
    "'''\n",
    "x0 = np.ones(shape=randomPointsVectors.shape[0])\n",
    "\n",
    "# append rows of ones that will be x0 for each vector\n",
    "# make XTrain into column vectors in the matrix\n",
    "XTrain = np.column_stack((x0, randomPointsVectors)).T\n",
    "\n",
    "# YTrain are the classification the point belongs to (1 | -1)\n",
    "YTrain = generatePLATestYs(w=targetFInVectorForm, x=XTrain)\n",
    "# points = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test PLA and get g to approximate f or equal\n",
    "plaModel = PLA(x=XTrain, y=YTrain)\n",
    "plaModel.train()\n",
    "g = plaModel.w\n",
    "plaModel.countWeightVectorUpdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = np.row_stack((XTrain, YTrain)).T\n",
    "fLineColor = \"#015220\"\n",
    "gLineColor = \"#8b0000\"\n",
    "\n",
    "\n",
    "# each row is a vector\n",
    "df = pd.DataFrame(source, columns=[\"x0\", \"x1\", \"x2\", \"class\"])\n",
    "\n",
    "# plot target F & hypothesis G function line: need to make dummy x data...\n",
    "plotXs = np.arange(-100, 100)\n",
    "\n",
    "# calculate y values for plotXs\n",
    "dfFLine = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": plotXs,\n",
    "        # aka y/target function\n",
    "        \"f(x)\": calculateYsForLine(plotXs, line)\n",
    "    }\n",
    ")\n",
    "\n",
    "# transform vector that is equation of a line in standard form to\n",
    "# slope intercept form y = mx + b\n",
    "# thus needed to solve for y by divining by the y's coefficient\n",
    "gLine = {\n",
    "    \"slope\": -(g[1][0]/g[2][0]),\n",
    "    \"yIntercept\": -(g[0][0]/g[2][0])\n",
    "}\n",
    "\n",
    "dfGLine = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": plotXs,\n",
    "        # aka g/hypothesis function\n",
    "        \"g(x)\": calculateYsForLine(plotXs, gLine)\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-db4da4149a204216b85bcbf3a5d5aa83\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-db4da4149a204216b85bcbf3a5d5aa83\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-db4da4149a204216b85bcbf3a5d5aa83\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-5f6ce3800bdddfc93e35498fb32eafde\"}, \"mark\": {\"type\": \"circle\", \"size\": 50}, \"encoding\": {\"color\": {\"field\": \"class\", \"scale\": {\"domain\": [-1, 1], \"range\": [\"red\", \"green\"]}, \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"x1\", \"title\": \"x\", \"type\": \"quantitative\"}, {\"field\": \"x2\", \"title\": \"y\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"x1\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"x2\", \"type\": \"quantitative\"}}, \"height\": 900, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 900}, {\"data\": {\"name\": \"data-f7f274894a63fa20e391cbc89c4149df\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"value\": \"#015220\"}, \"x\": {\"field\": \"x\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"f(x)\", \"type\": \"quantitative\"}}}, {\"data\": {\"name\": \"data-c64603a327e7616a140fbef008711079\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"value\": \"#8b0000\"}, \"x\": {\"field\": \"x\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"g(x)\", \"type\": \"quantitative\"}}}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-5f6ce3800bdddfc93e35498fb32eafde\": [{\"x0\": 1.0, \"x1\": 0.0976270078546495, \"x2\": 0.43037873274483895, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.20552675214328775, \"x2\": 0.08976636599379373, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.15269040132219058, \"x2\": 0.29178822613331223, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.12482557747461498, \"x2\": 0.7835460015641595, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9273255210020586, \"x2\": -0.2331169623484446, \"class\": -1.0}, {\"x0\": 1.0, \"x1\": 0.5834500761653292, \"x2\": 0.05778983950580896, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.13608912218786462, \"x2\": 0.8511932765853221, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.8579278836042261, \"x2\": -0.8257414005969186, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.9595632051193486, \"x2\": 0.665239691095876, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.556313501899701, \"x2\": 0.7400242964936383, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.957236684465528, \"x2\": 0.5983171284334472, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.07704127549413631, \"x2\": 0.5610583525729109, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.7634511482621336, \"x2\": 0.27984204265504764, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.7132934251819072, \"x2\": 0.8893378340991678, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.04369664350014335, \"x2\": -0.17067612001895283, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.47088877579074606, \"x2\": 0.5484673788684333, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.0876993355669029, \"x2\": 0.13686789773729702, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.9624203991272897, \"x2\": 0.23527099415175412, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.22419144544484282, \"x2\": 0.23386799374951384, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.8874961570292483, \"x2\": 0.3636405982069668, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.280984198852428, \"x2\": -0.1259360924013171, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3952623918545297, \"x2\": -0.8795490567414603, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.33353343089133536, \"x2\": 0.3412757392363188, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.5792348778523182, \"x2\": -0.7421474046902934, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.36914329815163227, \"x2\": -0.2725784581147548, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.14039354083575928, \"x2\": -0.1227969730753593, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9767476761184524, \"x2\": -0.7959103785039439, \"class\": -1.0}, {\"x0\": 1.0, \"x1\": -0.5822464878103306, \"x2\": -0.6773809642300075, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.30621665093079686, \"x2\": -0.4934167949204358, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.06737845428738742, \"x2\": -0.5111488159967945, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.6820608327089606, \"x2\": -0.7792497176713897, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.31265917893054684, \"x2\": -0.7236340973027724, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.606835276639893, \"x2\": -0.2625496586780718, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.6419864596958702, \"x2\": -0.8057974484138775, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.6758898149976078, \"x2\": -0.8078031842120739, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9529189300267915, \"x2\": -0.06269759670459685, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9535221763806743, \"x2\": 0.20969103949009193, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.47852715879660335, \"x2\": -0.9216244154913587, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.43438607484718084, \"x2\": -0.7596068775736622, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.40771960495571014, \"x2\": -0.7625445620915119, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.36403364121204795, \"x2\": -0.17147401097066006, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.8717050073024313, \"x2\": 0.3849442387400397, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.1332029084131503, \"x2\": -0.46922101812110917, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.04649610693339934, \"x2\": -0.8121189784831166, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.15189299111235854, \"x2\": 0.8585923951524281, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.3628620950973527, \"x2\": 0.3348207599273634, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.7364042751912157, \"x2\": 0.4326544082371311, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.4211878141055978, \"x2\": -0.6336172759857663, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.1730258696201663, \"x2\": -0.9597849076250129, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.6578800584347262, \"x2\": -0.9906090476149059, \"class\": -1.0}, {\"x0\": 1.0, \"x1\": 0.3556330735924602, \"x2\": -0.4599840536156703, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.47038804424518976, \"x2\": 0.9243770902348765, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.5024937129600839, \"x2\": 0.1523146688356738, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.18408386254367803, \"x2\": 0.14450381158174674, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.5538367347187634, \"x2\": 0.90549802303397, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.10574924276474529, \"x2\": 0.6928173449422557, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3989585506350086, \"x2\": -0.4051260982897327, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.6275956394049544, \"x2\": -0.2069885183060307, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.7622063942223232, \"x2\": 0.1625457452717174, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.7634707237097056, \"x2\": 0.38506318015553176, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.45050855963928105, \"x2\": 0.002648763853404512, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9121672694464478, \"x2\": 0.2879803984592748, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.15228990288364064, \"x2\": 0.2127864282558487, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.961613603381333, \"x2\": -0.39685036665090134, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3203470749853701, \"x2\": -0.41984478557911187, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.23603085799768309, \"x2\": -0.14246259810846773, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.7290518715550995, \"x2\": -0.40343534808793846, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.13992982140252974, \"x2\": 0.18174552249634646, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.1486504976991576, \"x2\": 0.30640163971426726, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.30420654000337777, \"x2\": -0.1371631291320521, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.793093191702126, \"x2\": -0.26487625990420693, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.12827014946874638, \"x2\": 0.7838467100313442, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.6123879780921715, \"x2\": 0.40777716708073264, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.7995462253753978, \"x2\": 0.8389652274893471, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.42848259909822284, \"x2\": 0.997694013135733, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.7011033906840125, \"x2\": 0.7362521147364285, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.6750141306472504, \"x2\": 0.23111912856768835, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.752360034301117, \"x2\": 0.6960164586444688, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.6146379174500214, \"x2\": 0.13820147722918663, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.18563340554800067, \"x2\": -0.8616660090897239, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3948575462891273, \"x2\": -0.09291463464386229, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.44411119894069584, \"x2\": 0.7327646518572584, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9510430100057716, \"x2\": 0.711606684785222, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.9765718316299961, \"x2\": -0.2800438710432722, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.45998112484811604, \"x2\": -0.656740645477119, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.042073212408258565, \"x2\": -0.8913240233214927, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.6000069502071999, \"x2\": -0.9629564110787721, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.5873954067148413, \"x2\": -0.5521506238792397, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.3092966386061946, \"x2\": 0.8561625869311817, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.40882880384706555, \"x2\": -0.9363221409373843, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.6706116870041745, \"x2\": 0.24295680299952704, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.15445717720833518, \"x2\": -0.5242143572509828, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.8684279958495875, \"x2\": 0.22793191193179196, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.07126560604991661, \"x2\": 0.17981995270914197, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.46024405903353927, \"x2\": -0.3761100090407963, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.20355787556781624, \"x2\": -0.5803125020497557, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.6276139882393277, \"x2\": 0.8887447799678672, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.4791015900985751, \"x2\": -0.019082382764865846, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.5451707440533535, \"x2\": -0.4912870364592141, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.8839416793522488, \"x2\": -0.13116674888375845, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.37640823601179485, \"x2\": 0.392686977630919, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.2444963214150382, \"x2\": -0.6407926448807304, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.9506425432173375, \"x2\": -0.8655007370735028, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3587855469971346, \"x2\": -0.09260631088790938, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.0731584222174444, \"x2\": 0.7933425860806842, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9806778947934087, \"x2\": -0.5662060312030521, \"class\": -1.0}, {\"x0\": 1.0, \"x1\": 0.3261564062002016, \"x2\": -0.4733552465256987, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.9586980010685426, \"x2\": 0.5167573076722829, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.3599656983550643, \"x2\": -0.23307221165620406, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.17663422710721144, \"x2\": 0.6620969104723808, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.25796368718229745, \"x2\": 0.7453013108947906, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.45291593036872846, \"x2\": 0.5960936678251274, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.6287281113880956, \"x2\": 0.9055833139438891, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3749765527756306, \"x2\": -0.5689846457728831, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.8947411809778485, \"x2\": 0.46171161354031565, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.49211671480994834, \"x2\": -0.573376045265036, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.036401427861326496, \"x2\": -0.9486745638909369, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.5850598491177812, \"x2\": -0.1506290624969875, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.2516600393315489, \"x2\": -0.07284915127037861, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.4447425874105362, \"x2\": 0.17356869291633759, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.7277112118464628, \"x2\": -0.7649362880759338, \"class\": -1.0}, {\"x0\": 1.0, \"x1\": 0.03475821430822834, \"x2\": -0.7358637873096934, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.4337193623851874, \"x2\": -0.2078805943854125, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.13084262371701794, \"x2\": -0.6334403275718428, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.7103044813132455, \"x2\": -0.02388743870209087, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.28877452430008876, \"x2\": 0.8808638905056261, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.5306505076139305, \"x2\": 0.4973272397010946, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.8074394794918669, \"x2\": -0.8331551291159629, \"class\": -1.0}, {\"x0\": 1.0, \"x1\": 0.10438493984481312, \"x2\": 0.16895213791153774, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.923872757094458, \"x2\": -0.4157049464149023, \"class\": -1.0}, {\"x0\": 1.0, \"x1\": -0.5183424401691064, \"x2\": -0.7994121154690044, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.9671407408170516, \"x2\": 0.8590586335843811, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3398330931818201, \"x2\": 0.5703058240462755, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.43653978849210184, \"x2\": 0.17282033237265337, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.8720894677580378, \"x2\": -0.028744808130754196, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9549902794888936, \"x2\": 0.7530104906331816, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.32368209632630873, \"x2\": 0.923140309082997, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.536596747057591, \"x2\": 0.8986376448313629, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.8827554094129972, \"x2\": 0.5984051747047834, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.26089587373358225, \"x2\": 0.748575933249894, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.4139594309844066, \"x2\": 0.6978871106258364, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.23575338383504763, \"x2\": -0.973526284482201, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.30553296413556086, \"x2\": -0.7037182781036699, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9636587796365064, \"x2\": -0.043259385920023874, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.005217269002674607, \"x2\": 0.2789450327974472, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.26283078774076496, \"x2\": -0.7261994566288021, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.644235466388491, \"x2\": -0.6203041761944841, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.02263796509291205, \"x2\": -0.5513659420505215, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.8043110310119319, \"x2\": 0.7243830348433666, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9458389780462606, \"x2\": 0.9216693161260003, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.8131109984423579, \"x2\": 0.5480946653972776, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.33370969594271616, \"x2\": -0.8377972200240065, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.18551765717238533, \"x2\": -0.5355317156581145, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.735024730484034, \"x2\": -0.8931456364263495, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.45118872842115754, \"x2\": -0.9771450827499379, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.5411614970055525, \"x2\": -0.7061067091992499, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.8409558348264885, \"x2\": -0.8207939315227892, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3440956147078289, \"x2\": -0.5092655802943105, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.1589210666398031, \"x2\": 0.11473758264783385, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.7211023476575875, \"x2\": 0.45408852542265654, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.4593441895225707, \"x2\": -0.7370344014177448, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.8892513591576041, \"x2\": -0.39680273103811503, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.4757637015206435, \"x2\": -0.0877188663990407, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3665626710953609, \"x2\": 0.3912508912777144, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.4329623068356667, \"x2\": -0.24014608819975902, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.6376980765261939, \"x2\": 0.5770910246130374, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.8863038471335194, \"x2\": 0.3939944834499747, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.5573907918822067, \"x2\": 0.5548151236975063, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.48115487130929013, \"x2\": -0.25237372413487713, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.17519927039277805, \"x2\": -0.45435619515106596, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.25829440156422256, \"x2\": -0.6058914396287207, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.08028823248798522, \"x2\": -0.9107753974917718, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.599591769141236, \"x2\": -0.8460871060267345, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.03767029766305208, \"x2\": -0.3863798009096078, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.15508589766275094, \"x2\": 0.9188666816668503, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.2911404889120077, \"x2\": -0.9292751284890182, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.13919512098387754, \"x2\": 0.02003370463650045, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.07235498940690399, \"x2\": 0.36278502120767575, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.44480780453646784, \"x2\": -0.7422788690673596, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.2146486469058113, \"x2\": 0.9128114455918976, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.6257382164983105, \"x2\": 0.807967909856474, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.0876119001546527, \"x2\": -0.08617715670846837, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.7640828204597792, \"x2\": -0.08279207646282827, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.44833527322308653, \"x2\": -0.20194935659379598, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.8080887858019155, \"x2\": 0.3800500403824547, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.3992441085010334, \"x2\": -0.3445591968857622, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.5135572854737784, \"x2\": 0.2721221108942826, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.5199594532405809, \"x2\": -0.6789223550294872, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.5927829490346634, \"x2\": 0.918333206070445, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.0837223454799143, \"x2\": 0.18196833064736984, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.7154452883871092, \"x2\": -0.08555309329228589, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.9037489536654724, \"x2\": 0.15150232408974484, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.6415342414026299, \"x2\": 0.8176874368254767, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.6310476375371377, \"x2\": -0.6811710731020881, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": 0.2577968781234008, \"x2\": -0.20313148276064585, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.8745740959533086, \"x2\": -0.1519354962203161, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.4826318662211846, \"x2\": 0.6980766168570216, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.9333907469066076, \"x2\": 0.9179654437269471, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.28926230305614076, \"x2\": -0.2865862191949142, \"class\": 1.0}, {\"x0\": 1.0, \"x1\": -0.9673429946325842, \"x2\": -0.6295353495276321, \"class\": 1.0}], \"data-f7f274894a63fa20e391cbc89c4149df\": [{\"x\": -100, \"f(x)\": -282.5788710714884}, {\"x\": -99, \"f(x)\": -279.7810180980425}, {\"x\": -98, \"f(x)\": -276.98316512459667}, {\"x\": -97, \"f(x)\": -274.1853121511508}, {\"x\": -96, \"f(x)\": -271.3874591777049}, {\"x\": -95, \"f(x)\": -268.58960620425904}, {\"x\": -94, \"f(x)\": -265.7917532308132}, {\"x\": -93, \"f(x)\": -262.99390025736733}, {\"x\": -92, \"f(x)\": -260.1960472839215}, {\"x\": -91, \"f(x)\": -257.3981943104756}, {\"x\": -90, \"f(x)\": -254.60034133702973}, {\"x\": -89, \"f(x)\": -251.80248836358388}, {\"x\": -88, \"f(x)\": -249.00463539013802}, {\"x\": -87, \"f(x)\": -246.20678241669216}, {\"x\": -86, \"f(x)\": -243.4089294432463}, {\"x\": -85, \"f(x)\": -240.61107646980045}, {\"x\": -84, \"f(x)\": -237.8132234963546}, {\"x\": -83, \"f(x)\": -235.01537052290874}, {\"x\": -82, \"f(x)\": -232.21751754946288}, {\"x\": -81, \"f(x)\": -229.41966457601703}, {\"x\": -80, \"f(x)\": -226.62181160257117}, {\"x\": -79, \"f(x)\": -223.8239586291253}, {\"x\": -78, \"f(x)\": -221.02610565567943}, {\"x\": -77, \"f(x)\": -218.22825268223357}, {\"x\": -76, \"f(x)\": -215.43039970878772}, {\"x\": -75, \"f(x)\": -212.63254673534186}, {\"x\": -74, \"f(x)\": -209.834693761896}, {\"x\": -73, \"f(x)\": -207.03684078845015}, {\"x\": -72, \"f(x)\": -204.2389878150043}, {\"x\": -71, \"f(x)\": -201.44113484155844}, {\"x\": -70, \"f(x)\": -198.64328186811258}, {\"x\": -69, \"f(x)\": -195.8454288946667}, {\"x\": -68, \"f(x)\": -193.04757592122084}, {\"x\": -67, \"f(x)\": -190.24972294777498}, {\"x\": -66, \"f(x)\": -187.45186997432913}, {\"x\": -65, \"f(x)\": -184.65401700088327}, {\"x\": -64, \"f(x)\": -181.85616402743742}, {\"x\": -63, \"f(x)\": -179.05831105399156}, {\"x\": -62, \"f(x)\": -176.2604580805457}, {\"x\": -61, \"f(x)\": -173.46260510709985}, {\"x\": -60, \"f(x)\": -170.664752133654}, {\"x\": -59, \"f(x)\": -167.86689916020813}, {\"x\": -58, \"f(x)\": -165.06904618676225}, {\"x\": -57, \"f(x)\": -162.2711932133164}, {\"x\": -56, \"f(x)\": -159.47334023987054}, {\"x\": -55, \"f(x)\": -156.67548726642468}, {\"x\": -54, \"f(x)\": -153.87763429297883}, {\"x\": -53, \"f(x)\": -151.07978131953297}, {\"x\": -52, \"f(x)\": -148.2819283460871}, {\"x\": -51, \"f(x)\": -145.48407537264126}, {\"x\": -50, \"f(x)\": -142.6862223991954}, {\"x\": -49, \"f(x)\": -139.88836942574954}, {\"x\": -48, \"f(x)\": -137.09051645230366}, {\"x\": -47, \"f(x)\": -134.2926634788578}, {\"x\": -46, \"f(x)\": -131.49481050541195}, {\"x\": -45, \"f(x)\": -128.6969575319661}, {\"x\": -44, \"f(x)\": -125.89910455852024}, {\"x\": -43, \"f(x)\": -123.10125158507438}, {\"x\": -42, \"f(x)\": -120.30339861162852}, {\"x\": -41, \"f(x)\": -117.50554563818267}, {\"x\": -40, \"f(x)\": -114.70769266473681}, {\"x\": -39, \"f(x)\": -111.90983969129094}, {\"x\": -38, \"f(x)\": -109.11198671784508}, {\"x\": -37, \"f(x)\": -106.31413374439923}, {\"x\": -36, \"f(x)\": -103.51628077095337}, {\"x\": -35, \"f(x)\": -100.71842779750752}, {\"x\": -34, \"f(x)\": -97.92057482406165}, {\"x\": -33, \"f(x)\": -95.12272185061579}, {\"x\": -32, \"f(x)\": -92.32486887716993}, {\"x\": -31, \"f(x)\": -89.52701590372408}, {\"x\": -30, \"f(x)\": -86.72916293027822}, {\"x\": -29, \"f(x)\": -83.93130995683235}, {\"x\": -28, \"f(x)\": -81.1334569833865}, {\"x\": -27, \"f(x)\": -78.33560400994064}, {\"x\": -26, \"f(x)\": -75.53775103649478}, {\"x\": -25, \"f(x)\": -72.73989806304893}, {\"x\": -24, \"f(x)\": -69.94204508960306}, {\"x\": -23, \"f(x)\": -67.1441921161572}, {\"x\": -22, \"f(x)\": -64.34633914271134}, {\"x\": -21, \"f(x)\": -61.54848616926549}, {\"x\": -20, \"f(x)\": -58.75063319581963}, {\"x\": -19, \"f(x)\": -55.95278022237377}, {\"x\": -18, \"f(x)\": -53.15492724892791}, {\"x\": -17, \"f(x)\": -50.35707427548205}, {\"x\": -16, \"f(x)\": -47.55922130203619}, {\"x\": -15, \"f(x)\": -44.761368328590336}, {\"x\": -14, \"f(x)\": -41.96351535514447}, {\"x\": -13, \"f(x)\": -39.16566238169862}, {\"x\": -12, \"f(x)\": -36.367809408252754}, {\"x\": -11, \"f(x)\": -33.5699564348069}, {\"x\": -10, \"f(x)\": -30.772103461361045}, {\"x\": -9, \"f(x)\": -27.974250487915185}, {\"x\": -8, \"f(x)\": -25.176397514469326}, {\"x\": -7, \"f(x)\": -22.378544541023466}, {\"x\": -6, \"f(x)\": -19.580691567577606}, {\"x\": -5, \"f(x)\": -16.78283859413175}, {\"x\": -4, \"f(x)\": -13.98498562068589}, {\"x\": -3, \"f(x)\": -11.18713264724003}, {\"x\": -2, \"f(x)\": -8.38927967379417}, {\"x\": -1, \"f(x)\": -5.591426700348313}, {\"x\": 0, \"f(x)\": -2.793573726902454}, {\"x\": 1, \"f(x)\": 0.0042792465434047244}, {\"x\": 2, \"f(x)\": 2.8021322199892635}, {\"x\": 3, \"f(x)\": 5.5999851934351215}, {\"x\": 4, \"f(x)\": 8.39783816688098}, {\"x\": 5, \"f(x)\": 11.19569114032684}, {\"x\": 6, \"f(x)\": 13.993544113772696}, {\"x\": 7, \"f(x)\": 16.791397087218556}, {\"x\": 8, \"f(x)\": 19.589250060664416}, {\"x\": 9, \"f(x)\": 22.387103034110275}, {\"x\": 10, \"f(x)\": 25.184956007556135}, {\"x\": 11, \"f(x)\": 27.98280898100199}, {\"x\": 12, \"f(x)\": 30.780661954447847}, {\"x\": 13, \"f(x)\": 33.578514927893714}, {\"x\": 14, \"f(x)\": 36.37636790133957}, {\"x\": 15, \"f(x)\": 39.17422087478543}, {\"x\": 16, \"f(x)\": 41.97207384823129}, {\"x\": 17, \"f(x)\": 44.769926821677146}, {\"x\": 18, \"f(x)\": 47.56777979512301}, {\"x\": 19, \"f(x)\": 50.365632768568865}, {\"x\": 20, \"f(x)\": 53.16348574201473}, {\"x\": 21, \"f(x)\": 55.961338715460585}, {\"x\": 22, \"f(x)\": 58.75919168890644}, {\"x\": 23, \"f(x)\": 61.5570446623523}, {\"x\": 24, \"f(x)\": 64.35489763579815}, {\"x\": 25, \"f(x)\": 67.15275060924402}, {\"x\": 26, \"f(x)\": 69.95060358268988}, {\"x\": 27, \"f(x)\": 72.74845655613574}, {\"x\": 28, \"f(x)\": 75.54630952958159}, {\"x\": 29, \"f(x)\": 78.34416250302745}, {\"x\": 30, \"f(x)\": 81.14201547647332}, {\"x\": 31, \"f(x)\": 83.93986844991917}, {\"x\": 32, \"f(x)\": 86.73772142336503}, {\"x\": 33, \"f(x)\": 89.53557439681089}, {\"x\": 34, \"f(x)\": 92.33342737025674}, {\"x\": 35, \"f(x)\": 95.13128034370261}, {\"x\": 36, \"f(x)\": 97.92913331714847}, {\"x\": 37, \"f(x)\": 100.72698629059433}, {\"x\": 38, \"f(x)\": 103.52483926404018}, {\"x\": 39, \"f(x)\": 106.32269223748604}, {\"x\": 40, \"f(x)\": 109.12054521093191}, {\"x\": 41, \"f(x)\": 111.91839818437776}, {\"x\": 42, \"f(x)\": 114.71625115782362}, {\"x\": 43, \"f(x)\": 117.51410413126948}, {\"x\": 44, \"f(x)\": 120.31195710471533}, {\"x\": 45, \"f(x)\": 123.10981007816119}, {\"x\": 46, \"f(x)\": 125.90766305160705}, {\"x\": 47, \"f(x)\": 128.7055160250529}, {\"x\": 48, \"f(x)\": 131.50336899849876}, {\"x\": 49, \"f(x)\": 134.30122197194464}, {\"x\": 50, \"f(x)\": 137.0990749453905}, {\"x\": 51, \"f(x)\": 139.89692791883635}, {\"x\": 52, \"f(x)\": 142.6947808922822}, {\"x\": 53, \"f(x)\": 145.49263386572807}, {\"x\": 54, \"f(x)\": 148.29048683917392}, {\"x\": 55, \"f(x)\": 151.08833981261978}, {\"x\": 56, \"f(x)\": 153.88619278606564}, {\"x\": 57, \"f(x)\": 156.6840457595115}, {\"x\": 58, \"f(x)\": 159.48189873295735}, {\"x\": 59, \"f(x)\": 162.27975170640323}, {\"x\": 60, \"f(x)\": 165.0776046798491}, {\"x\": 61, \"f(x)\": 167.87545765329494}, {\"x\": 62, \"f(x)\": 170.6733106267408}, {\"x\": 63, \"f(x)\": 173.47116360018666}, {\"x\": 64, \"f(x)\": 176.2690165736325}, {\"x\": 65, \"f(x)\": 179.06686954707837}, {\"x\": 66, \"f(x)\": 181.86472252052423}, {\"x\": 67, \"f(x)\": 184.66257549397008}, {\"x\": 68, \"f(x)\": 187.46042846741594}, {\"x\": 69, \"f(x)\": 190.2582814408618}, {\"x\": 70, \"f(x)\": 193.05613441430768}, {\"x\": 71, \"f(x)\": 195.85398738775353}, {\"x\": 72, \"f(x)\": 198.6518403611994}, {\"x\": 73, \"f(x)\": 201.44969333464525}, {\"x\": 74, \"f(x)\": 204.2475463080911}, {\"x\": 75, \"f(x)\": 207.04539928153696}, {\"x\": 76, \"f(x)\": 209.84325225498281}, {\"x\": 77, \"f(x)\": 212.64110522842867}, {\"x\": 78, \"f(x)\": 215.43895820187453}, {\"x\": 79, \"f(x)\": 218.23681117532038}, {\"x\": 80, \"f(x)\": 221.03466414876627}, {\"x\": 81, \"f(x)\": 223.83251712221212}, {\"x\": 82, \"f(x)\": 226.63037009565798}, {\"x\": 83, \"f(x)\": 229.42822306910384}, {\"x\": 84, \"f(x)\": 232.2260760425497}, {\"x\": 85, \"f(x)\": 235.02392901599555}, {\"x\": 86, \"f(x)\": 237.8217819894414}, {\"x\": 87, \"f(x)\": 240.61963496288726}, {\"x\": 88, \"f(x)\": 243.41748793633312}, {\"x\": 89, \"f(x)\": 246.21534090977897}, {\"x\": 90, \"f(x)\": 249.01319388322483}, {\"x\": 91, \"f(x)\": 251.8110468566707}, {\"x\": 92, \"f(x)\": 254.60889983011654}, {\"x\": 93, \"f(x)\": 257.40675280356237}, {\"x\": 94, \"f(x)\": 260.2046057770082}, {\"x\": 95, \"f(x)\": 263.0024587504541}, {\"x\": 96, \"f(x)\": 265.80031172389994}, {\"x\": 97, \"f(x)\": 268.59816469734585}, {\"x\": 98, \"f(x)\": 271.3960176707917}, {\"x\": 99, \"f(x)\": 274.19387064423756}], \"data-c64603a327e7616a140fbef008711079\": [{\"x\": -100, \"g(x)\": -299.04665362941023}, {\"x\": -99, \"g(x)\": -296.08531020222074}, {\"x\": -98, \"g(x)\": -293.12396677503125}, {\"x\": -97, \"g(x)\": -290.1626233478418}, {\"x\": -96, \"g(x)\": -287.2012799206523}, {\"x\": -95, \"g(x)\": -284.23993649346284}, {\"x\": -94, \"g(x)\": -281.27859306627334}, {\"x\": -93, \"g(x)\": -278.31724963908385}, {\"x\": -92, \"g(x)\": -275.35590621189436}, {\"x\": -91, \"g(x)\": -272.39456278470493}, {\"x\": -90, \"g(x)\": -269.43321935751544}, {\"x\": -89, \"g(x)\": -266.47187593032595}, {\"x\": -88, \"g(x)\": -263.51053250313646}, {\"x\": -87, \"g(x)\": -260.54918907594697}, {\"x\": -86, \"g(x)\": -257.5878456487575}, {\"x\": -85, \"g(x)\": -254.626502221568}, {\"x\": -84, \"g(x)\": -251.66515879437853}, {\"x\": -83, \"g(x)\": -248.70381536718904}, {\"x\": -82, \"g(x)\": -245.74247193999958}, {\"x\": -81, \"g(x)\": -242.78112851281008}, {\"x\": -80, \"g(x)\": -239.8197850856206}, {\"x\": -79, \"g(x)\": -236.85844165843113}, {\"x\": -78, \"g(x)\": -233.89709823124164}, {\"x\": -77, \"g(x)\": -230.93575480405215}, {\"x\": -76, \"g(x)\": -227.9744113768627}, {\"x\": -75, \"g(x)\": -225.0130679496732}, {\"x\": -74, \"g(x)\": -222.0517245224837}, {\"x\": -73, \"g(x)\": -219.09038109529425}, {\"x\": -72, \"g(x)\": -216.12903766810476}, {\"x\": -71, \"g(x)\": -213.16769424091527}, {\"x\": -70, \"g(x)\": -210.2063508137258}, {\"x\": -69, \"g(x)\": -207.24500738653632}, {\"x\": -68, \"g(x)\": -204.28366395934685}, {\"x\": -67, \"g(x)\": -201.32232053215736}, {\"x\": -66, \"g(x)\": -198.36097710496787}, {\"x\": -65, \"g(x)\": -195.3996336777784}, {\"x\": -64, \"g(x)\": -192.43829025058892}, {\"x\": -63, \"g(x)\": -189.47694682339943}, {\"x\": -62, \"g(x)\": -186.51560339620997}, {\"x\": -61, \"g(x)\": -183.55425996902048}, {\"x\": -60, \"g(x)\": -180.592916541831}, {\"x\": -59, \"g(x)\": -177.63157311464153}, {\"x\": -58, \"g(x)\": -174.67022968745204}, {\"x\": -57, \"g(x)\": -171.70888626026257}, {\"x\": -56, \"g(x)\": -168.74754283307308}, {\"x\": -55, \"g(x)\": -165.7861994058836}, {\"x\": -54, \"g(x)\": -162.82485597869413}, {\"x\": -53, \"g(x)\": -159.86351255150464}, {\"x\": -52, \"g(x)\": -156.90216912431515}, {\"x\": -51, \"g(x)\": -153.9408256971257}, {\"x\": -50, \"g(x)\": -150.9794822699362}, {\"x\": -49, \"g(x)\": -148.0181388427467}, {\"x\": -48, \"g(x)\": -145.05679541555725}, {\"x\": -47, \"g(x)\": -142.09545198836776}, {\"x\": -46, \"g(x)\": -139.13410856117827}, {\"x\": -45, \"g(x)\": -136.1727651339888}, {\"x\": -44, \"g(x)\": -133.2114217067993}, {\"x\": -43, \"g(x)\": -130.25007827960985}, {\"x\": -42, \"g(x)\": -127.28873485242036}, {\"x\": -41, \"g(x)\": -124.32739142523089}, {\"x\": -40, \"g(x)\": -121.3660479980414}, {\"x\": -39, \"g(x)\": -118.40470457085192}, {\"x\": -38, \"g(x)\": -115.44336114366244}, {\"x\": -37, \"g(x)\": -112.48201771647295}, {\"x\": -36, \"g(x)\": -109.52067428928348}, {\"x\": -35, \"g(x)\": -106.559330862094}, {\"x\": -34, \"g(x)\": -103.59798743490452}, {\"x\": -33, \"g(x)\": -100.63664400771503}, {\"x\": -32, \"g(x)\": -97.67530058052556}, {\"x\": -31, \"g(x)\": -94.71395715333608}, {\"x\": -30, \"g(x)\": -91.75261372614659}, {\"x\": -29, \"g(x)\": -88.79127029895712}, {\"x\": -28, \"g(x)\": -85.82992687176764}, {\"x\": -27, \"g(x)\": -82.86858344457816}, {\"x\": -26, \"g(x)\": -79.90724001738867}, {\"x\": -25, \"g(x)\": -76.9458965901992}, {\"x\": -24, \"g(x)\": -73.98455316300972}, {\"x\": -23, \"g(x)\": -71.02320973582023}, {\"x\": -22, \"g(x)\": -68.06186630863075}, {\"x\": -21, \"g(x)\": -65.10052288144128}, {\"x\": -20, \"g(x)\": -62.1391794542518}, {\"x\": -19, \"g(x)\": -59.17783602706233}, {\"x\": -18, \"g(x)\": -56.216492599872836}, {\"x\": -17, \"g(x)\": -53.25514917268336}, {\"x\": -16, \"g(x)\": -50.293805745493884}, {\"x\": -15, \"g(x)\": -47.332462318304394}, {\"x\": -14, \"g(x)\": -44.37111889111492}, {\"x\": -13, \"g(x)\": -41.40977546392544}, {\"x\": -12, \"g(x)\": -38.448432036735966}, {\"x\": -11, \"g(x)\": -35.487088609546475}, {\"x\": -10, \"g(x)\": -32.525745182357}, {\"x\": -9, \"g(x)\": -29.56440175516752}, {\"x\": -8, \"g(x)\": -26.60305832797804}, {\"x\": -7, \"g(x)\": -23.64171490078856}, {\"x\": -6, \"g(x)\": -20.68037147359908}, {\"x\": -5, \"g(x)\": -17.719028046409598}, {\"x\": -4, \"g(x)\": -14.75768461922012}, {\"x\": -3, \"g(x)\": -11.79634119203064}, {\"x\": -2, \"g(x)\": -8.834997764841159}, {\"x\": -1, \"g(x)\": -5.873654337651679}, {\"x\": 0, \"g(x)\": -2.9123109104621996}, {\"x\": 1, \"g(x)\": 0.049032516727280484}, {\"x\": 2, \"g(x)\": 3.0103759439167606}, {\"x\": 3, \"g(x)\": 5.971719371106241}, {\"x\": 4, \"g(x)\": 8.93306279829572}, {\"x\": 5, \"g(x)\": 11.8944062254852}, {\"x\": 6, \"g(x)\": 14.855749652674682}, {\"x\": 7, \"g(x)\": 17.81709307986416}, {\"x\": 8, \"g(x)\": 20.77843650705364}, {\"x\": 9, \"g(x)\": 23.73977993424312}, {\"x\": 10, \"g(x)\": 26.7011233614326}, {\"x\": 11, \"g(x)\": 29.66246678862208}, {\"x\": 12, \"g(x)\": 32.62381021581156}, {\"x\": 13, \"g(x)\": 35.585153643001036}, {\"x\": 14, \"g(x)\": 38.546497070190526}, {\"x\": 15, \"g(x)\": 41.50784049738}, {\"x\": 16, \"g(x)\": 44.46918392456948}, {\"x\": 17, \"g(x)\": 47.43052735175897}, {\"x\": 18, \"g(x)\": 50.391870778948444}, {\"x\": 19, \"g(x)\": 53.35321420613792}, {\"x\": 20, \"g(x)\": 56.3145576333274}, {\"x\": 21, \"g(x)\": 59.27590106051689}, {\"x\": 22, \"g(x)\": 62.23724448770636}, {\"x\": 23, \"g(x)\": 65.19858791489584}, {\"x\": 24, \"g(x)\": 68.15993134208533}, {\"x\": 25, \"g(x)\": 71.1212747692748}, {\"x\": 26, \"g(x)\": 74.08261819646428}, {\"x\": 27, \"g(x)\": 77.04396162365377}, {\"x\": 28, \"g(x)\": 80.00530505084325}, {\"x\": 29, \"g(x)\": 82.96664847803272}, {\"x\": 30, \"g(x)\": 85.9279919052222}, {\"x\": 31, \"g(x)\": 88.88933533241169}, {\"x\": 32, \"g(x)\": 91.85067875960117}, {\"x\": 33, \"g(x)\": 94.81202218679064}, {\"x\": 34, \"g(x)\": 97.77336561398013}, {\"x\": 35, \"g(x)\": 100.73470904116961}, {\"x\": 36, \"g(x)\": 103.69605246835908}, {\"x\": 37, \"g(x)\": 106.65739589554856}, {\"x\": 38, \"g(x)\": 109.61873932273805}, {\"x\": 39, \"g(x)\": 112.58008274992753}, {\"x\": 40, \"g(x)\": 115.541426177117}, {\"x\": 41, \"g(x)\": 118.5027696043065}, {\"x\": 42, \"g(x)\": 121.46411303149597}, {\"x\": 43, \"g(x)\": 124.42545645868545}, {\"x\": 44, \"g(x)\": 127.38679988587492}, {\"x\": 45, \"g(x)\": 130.3481433130644}, {\"x\": 46, \"g(x)\": 133.30948674025387}, {\"x\": 47, \"g(x)\": 136.27083016744336}, {\"x\": 48, \"g(x)\": 139.23217359463285}, {\"x\": 49, \"g(x)\": 142.19351702182232}, {\"x\": 50, \"g(x)\": 145.1548604490118}, {\"x\": 51, \"g(x)\": 148.1162038762013}, {\"x\": 52, \"g(x)\": 151.07754730339076}, {\"x\": 53, \"g(x)\": 154.03889073058025}, {\"x\": 54, \"g(x)\": 157.00023415776974}, {\"x\": 55, \"g(x)\": 159.9615775849592}, {\"x\": 56, \"g(x)\": 162.9229210121487}, {\"x\": 57, \"g(x)\": 165.88426443933818}, {\"x\": 58, \"g(x)\": 168.84560786652764}, {\"x\": 59, \"g(x)\": 171.80695129371713}, {\"x\": 60, \"g(x)\": 174.7682947209066}, {\"x\": 61, \"g(x)\": 177.7296381480961}, {\"x\": 62, \"g(x)\": 180.69098157528558}, {\"x\": 63, \"g(x)\": 183.65232500247504}, {\"x\": 64, \"g(x)\": 186.61366842966453}, {\"x\": 65, \"g(x)\": 189.57501185685402}, {\"x\": 66, \"g(x)\": 192.53635528404348}, {\"x\": 67, \"g(x)\": 195.49769871123297}, {\"x\": 68, \"g(x)\": 198.45904213842246}, {\"x\": 69, \"g(x)\": 201.42038556561192}, {\"x\": 70, \"g(x)\": 204.3817289928014}, {\"x\": 71, \"g(x)\": 207.34307241999088}, {\"x\": 72, \"g(x)\": 210.30441584718037}, {\"x\": 73, \"g(x)\": 213.26575927436986}, {\"x\": 74, \"g(x)\": 216.22710270155932}, {\"x\": 75, \"g(x)\": 219.1884461287488}, {\"x\": 76, \"g(x)\": 222.1497895559383}, {\"x\": 77, \"g(x)\": 225.11113298312776}, {\"x\": 78, \"g(x)\": 228.07247641031725}, {\"x\": 79, \"g(x)\": 231.03381983750674}, {\"x\": 80, \"g(x)\": 233.9951632646962}, {\"x\": 81, \"g(x)\": 236.9565066918857}, {\"x\": 82, \"g(x)\": 239.91785011907518}, {\"x\": 83, \"g(x)\": 242.87919354626464}, {\"x\": 84, \"g(x)\": 245.84053697345414}, {\"x\": 85, \"g(x)\": 248.8018804006436}, {\"x\": 86, \"g(x)\": 251.7632238278331}, {\"x\": 87, \"g(x)\": 254.72456725502255}, {\"x\": 88, \"g(x)\": 257.685910682212}, {\"x\": 89, \"g(x)\": 260.6472541094015}, {\"x\": 90, \"g(x)\": 263.608597536591}, {\"x\": 91, \"g(x)\": 266.5699409637805}, {\"x\": 92, \"g(x)\": 269.5312843909699}, {\"x\": 93, \"g(x)\": 272.4926278181594}, {\"x\": 94, \"g(x)\": 275.4539712453489}, {\"x\": 95, \"g(x)\": 278.4153146725384}, {\"x\": 96, \"g(x)\": 281.3766580997279}, {\"x\": 97, \"g(x)\": 284.33800152691737}, {\"x\": 98, \"g(x)\": 287.2993449541068}, {\"x\": 99, \"g(x)\": 290.2606883812963}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = alt.Chart(df.copy()).mark_circle(size=50).encode(\n",
    "    x=\"x1:Q\",\n",
    "    y=\"x2:Q\",\n",
    "    color= alt.Color(\"class\", scale=alt.Scale(domain=[-1,1], range=[\"red\", \"green\"])),\n",
    "    tooltip=[alt.Tooltip(\"x1:Q\", title=\"x\"), alt.Tooltip(\"x2:Q\", title=\"y\")]\n",
    ").properties(\n",
    "    height=900,\n",
    "    width=900\n",
    ").interactive()\n",
    "\n",
    "linePlot = alt.Chart(dfFLine.copy()).mark_line().encode(\n",
    "    x=\"x\",\n",
    "    y=\"f(x)\",\n",
    "    color=alt.value(fLineColor)\n",
    ")\n",
    "\n",
    "# plot g hypothesis hopefully approximates f well...\n",
    "\n",
    "gLinePlot = alt.Chart(dfGLine.copy()).mark_line().encode(\n",
    "    x=\"x\",\n",
    "    y=\"g(x)\",\n",
    "    color=alt.value(gLineColor)\n",
    ")\n",
    "\n",
    "points + linePlot + gLinePlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7: my HW Answer = b (I was correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runPLAExperiment(1000, 10, 1000, -1, 1, 2)\n",
    "# average in a run was 13.722\n",
    "# |13.722 - 15| = 1.278 this is the closest to zero out of the 5 choices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8: my HW Answer = c (I was correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runPLAExperiment(1000, 10, 1000, -1, 1, 2)\n",
    "# the probability that f and g will disagree on their classification of a random point = 0.114\n",
    "# |0.114 - 0.1| = 0.014 this is the closest to zero out of the 5 choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9: my HW Answer = b (I was correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runPLAExperiment(1000, 100, 1000, -1, 1, 2)\n",
    "# average in a run was 251.491\n",
    "# |251.491 - 100| = 1.278 this is the closest to zero out of the 5 choices\n",
    "\n",
    "'''\n",
    "The more data point we have in a set the longer the model has to train.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10: my HW Answer = b (I was correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183.664, 0.011802000000000003)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# runPLAExperiment(1000, 100, 1000, -1, 1, 2)\n",
    "# the probability that f and g will disagree on their classification of a random point = 0.114\n",
    "# |0.011802000000000003 - 0.01| = 0.001802000000000003 this is the closest to zero out of the 5 choices\n",
    "\n",
    "'''\n",
    "this makes sense because the more data points used to train the model, g, the better g approximates f.\n",
    "This means that there is less of a chance that g will make an \"error\".\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('sjsu-research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5822147d25398cc109c1bdf54eb1ee7a6c0faef180858af0c55c5362b83b772"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
